{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "In God We Trust\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE417: Artificial Intelligence\n",
    "\n",
    "Dr. Mahdiyeh Soleymani Baghshah\n",
    "\n",
    "Computer Engineering Department,\n",
    "Sharif University of Technology,\n",
    "Tehran, Tehran, Iran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (30 Points)\n",
    "\n",
    "HW3 :: Practical Question 2\n",
    "\n",
    "<br>\n",
    "\n",
    "Corresponding TA: Aryan Ahadinia, Saeed Hedayatiyan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, We're going to implement a generic MDP solver, so we can solve every question by passing transition and reward functions. We use an example to test our implementation but it's important to write MDP solver functions in a generic form. So it's important to maintain notebook structure and only write codes and comment in annotated areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling a Problem (5 Points)\n",
    "\n",
    "In a TV quiz show, there are several levels. At each level, if the participant answers the question correctly, they will receive some prize. If the participant's answer is wrong, they leave the competition empty-handed. Before each stage begins, the participant can decide whether to continue or withdraw and leave the game with the reward which they already earned.\n",
    "\n",
    "Beside states representing each level, There are three terminal states of Win, Lost and Quit in the game. Actions in each state are quit and play. The player will go to the quit state with probability of 1 if they decide to take action quit. otherwise they will pass the level i by probability of win_ratio[i] and go to the state which represents the next level.\n",
    "\n",
    "So the play / quit decision problem can be modeled as an MDP as below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<image src=\"./assets/quiz_problem.png\">\n",
    "</p>\n",
    "\n",
    "Considering $100, $200, $300, $400 and $500 as rewards and 0.9, 0.7, 0.6, 0.3, 0.1 as win ratio for levels 0 to 4 respectively.\n",
    "\n",
    "Complete two cells below to model the problem as an MDP. Write levels and actions as an str.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_levels = [\n",
    "    ####### Complete this list (0.25 Point) #######\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\"\n",
    "]\n",
    "quiz_terminals = [\n",
    "    ####### Complete this list (0.25 Point) #######\n",
    "    \"Lost\", \"Quit\", \"Win\"\n",
    "]\n",
    "\n",
    "quiz_states = quiz_levels + quiz_terminals\n",
    "quiz_actions = [\n",
    "    ####### Complete this list (0.5 Point) #######\n",
    "    \"play\", \"quit\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz_transition(state: str, action: str, next_state: str) -> float:\n",
    "    ###########################################\n",
    "    ########## Code Here! (2 Points) ##########\n",
    "    ###########################################\n",
    "    if action == \"quit\":\n",
    "        if next_state == \"Quit\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if state == \"0\":\n",
    "        if next_state == \"1\":\n",
    "            return 0.9\n",
    "        elif next_state == \"Lost\":\n",
    "            return 0.1\n",
    "\n",
    "    if state == \"1\":\n",
    "        if next_state == \"2\":\n",
    "            return 0.7\n",
    "        elif next_state == \"Lost\":\n",
    "            return 0.3\n",
    "\n",
    "    if state == \"2\":\n",
    "        if next_state == \"3\":\n",
    "            return 0.6\n",
    "        elif next_state == \"Lost\":\n",
    "            return 0.4\n",
    "\n",
    "    if state == \"3\":\n",
    "        if next_state == \"4\":\n",
    "            return 0.3\n",
    "        elif next_state == \"Lost\":\n",
    "            return 0.7\n",
    "\n",
    "    if state == \"4\":\n",
    "        if next_state == \"Win\":\n",
    "            return 0.1\n",
    "        elif next_state == \"Lost\":\n",
    "            return 0.9\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def quiz_reward(state: str, action: str, next_state: str) -> Union[float, int]:\n",
    "    ###########################################\n",
    "    ########## Code Here! (2 Points) ##########\n",
    "    ###########################################\n",
    "    if action == \"quit\":\n",
    "        if next_state == \"Quit\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if state == \"0\":\n",
    "        if next_state == \"1\":\n",
    "            return 100\n",
    "        elif next_state == \"Lost\":\n",
    "            return 0\n",
    "\n",
    "    if state == \"1\":\n",
    "        if next_state == \"2\":\n",
    "            return 200\n",
    "        elif next_state == \"Lost\":\n",
    "            return -100\n",
    "\n",
    "    if state == \"2\":\n",
    "        if next_state == \"3\":\n",
    "            return 300\n",
    "        elif next_state == \"Lost\":\n",
    "            return -300\n",
    "\n",
    "    if state == \"3\":\n",
    "        if next_state == \"4\":\n",
    "            return 400\n",
    "        elif next_state == \"Lost\":\n",
    "            return -600\n",
    "\n",
    "    if state == \"4\":\n",
    "        if next_state == \"Win\":\n",
    "            return 500\n",
    "        elif next_state == \"Lost\":\n",
    "            return -1000\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration (10 Points)\n",
    "\n",
    "Now we want to implement a function which does the value iteration process. Consider that this function must be a generic function to all MDP problems. So any internal reference to global variables is forbidden.\n",
    "\n",
    "This function gets transition function, reward function, discounting factor (gamma), list of states, list of terminal states, list of all possible actions and current value of each state as input and must return two lists: (1) updated values of each state and (2) list of best action which can be taken in each state regarding current values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_iterate(\n",
    "    transition_function: Callable[[str, str, str], float],\n",
    "    reward_function: Callable[[str, str, str], float],\n",
    "    gamma: float,\n",
    "    states: List[str],\n",
    "    terminals: List[str],\n",
    "    actions: List[str],\n",
    "    current_values: List[Union[float, int]],\n",
    ") -> Tuple[List[Union[float, int]]]:\n",
    "    new_values = []\n",
    "    best_actions = []\n",
    "    ###########################################\n",
    "    ########## Code Here! (10 Points) #########\n",
    "    ###########################################\n",
    "    \n",
    "    for state in states:\n",
    "        if state in terminals:\n",
    "            new_values.append(0)\n",
    "            continue\n",
    "        v_s = 0\n",
    "        best_action_value = -100000\n",
    "        best_action = 0\n",
    "        for action in actions:\n",
    "            action_value = 0\n",
    "            for i in range(len(states)):\n",
    "                new_state = states[i]\n",
    "                p = transition_function(state, action, new_state)\n",
    "                r = reward_function(state, action, new_state)\n",
    "                action_value = action_value + p * (current_values[i] + (1 / gamma) ** i * r)\n",
    "            if action_value >= best_action_value:\n",
    "                best_action_value = action_value\n",
    "                best_action = action\n",
    "\n",
    "        new_values.append(v_s + best_action_value)\n",
    "        best_actions.append(best_action)\n",
    "\n",
    "\n",
    "\n",
    "    return new_values, best_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Solving (5 Points)\n",
    "\n",
    "Now write a function to use the iterative approach to calculate values of each state and solve the MDP problem.\n",
    "\n",
    "This function gets transition function, reward function, discounting factor (gamma), list of states, list of terminal states, list of all possible actions and number of iterations as input and must return a dictionary which maps states to best action can be taken. For example:\n",
    "\n",
    "```Python\n",
    "{\n",
    "    'S1': 'A1',\n",
    "    'S2': 'A2',\n",
    "    ...\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def mdp_solve(\n",
    "    transition_function: Callable[[str, str, str], float],\n",
    "    reward_function: Callable[[str, str, str], float],\n",
    "    gamma: float,\n",
    "    states: List[str],\n",
    "    terminals: List[str],\n",
    "    actions: List[str],\n",
    "    iter: int,\n",
    ") -> Dict['str', 'str']:\n",
    "    ###########################################\n",
    "    ########## Code Here! (5 Points) #########\n",
    "    ###########################################\n",
    "    cur_values = [0 for i in range(len(states))]\n",
    "    best = dict()\n",
    "    for i in range(iter):\n",
    "        cur_values, actions = mdp_iterate(transition_function, reward_function, gamma, states, terminals, actions, cur_values)\n",
    "    for i in range(len(actions)):\n",
    "        if states[i] in terminals:\n",
    "            continue\n",
    "        best.update({states[i]: actions[i]})\n",
    "    return best  # Change this line to return the correct value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, running this code must solve the problem described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'play', '1': 'play', '2': 'play', '3': 'quit', '4': 'quit'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_solve(\n",
    "    quiz_transition,\n",
    "    quiz_reward,\n",
    "    1,\n",
    "    quiz_states,\n",
    "    quiz_terminals,\n",
    "    quiz_actions,\n",
    "    1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing effect of discounting factor (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, solve the problem for different values of discounting factor. Then draw a plot to show how increasing discounting factor effect on best decision that should be take. Then write a paragraph about the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Policy for gamma = 1\n",
      "{'0': 'play', '1': 'play', '2': 'play', '3': 'quit', '4': 'quit'} \n",
      "\n",
      "Best Policy for gamma = 0.8\n",
      "{'0': 'play', '1': 'play', '2': 'quit', '3': 'quit', '4': 'quit'} \n",
      "\n",
      "Best Policy for gamma = 0.5\n",
      "{'0': 'play', '1': 'quit', '2': 'quit', '3': 'quit', '4': 'quit'} \n",
      "\n",
      "Best Policy for gamma = 0.25\n",
      "{'0': 'play', '1': 'quit', '2': 'quit', '3': 'quit', '4': 'quit'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "########## Code Here! (5 Points) #########\n",
    "###########################################\n",
    "gammas = [1, 0.8, 0.5, 0.25]\n",
    "for gamma in gammas:\n",
    "    print(\"Best Policy for gamma =\", gamma)\n",
    "    best_actions = mdp_solve(\n",
    "                    quiz_transition,\n",
    "                    quiz_reward,\n",
    "                    gamma,\n",
    "                    quiz_states,\n",
    "                    quiz_terminals,\n",
    "                    quiz_actions,\n",
    "                    1000,\n",
    "                    )\n",
    "    print(best_actions, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######### Write Here (5 Points) ########### </br>\n",
    "As we see above, the less $\\gamma$ becomes, the sooner we quit the game; because by decreasing $\\gamma$, we essentially decrease the the reward in later steps, so expected reward of quitting sooner becomes more promissing. In fact, by decreasing $\\gamma$, we choose a low-risk low-reward strategy.\n",
    "For $\\gamma = 1$, we quit at the 3rd level, for $\\gamma = 0.8$, we quit at the 2nd level and for $\\gamma = 0.5$, we quit at the 1st level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d52d16d036a1482ffe774c8f1e76ae8e4e0f0b872023686895b726c9c226e62e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
